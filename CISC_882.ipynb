{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 22:04:11.363769: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-14 22:04:11.380100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734213851.398453   14089 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734213851.404016   14089 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-14 22:04:11.423282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import patient_data\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the folder path for the cancer and the non-cancer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = json.loads(open(\"./paths.json\").read())\n",
    "\n",
    "personal_path = all_paths['personal_path']\n",
    "cancerous_path = personal_path + all_paths['cancerous_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure GPUs is applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Num GPUs Available: ', len(physical_devices))\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the DICOM files and preprocess/label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m             patients[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlabel_imgs()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m patients\n\u001b[0;32m---> 14\u001b[0m patients \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_patients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcancerous_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mload_all_patients\u001b[0;34m(path, add_label)\u001b[0m\n\u001b[1;32m      4\u001b[0m folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m folder:\n\u001b[0;32m----> 6\u001b[0m     patients\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(patients, \u001b[43mpatient_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPatient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m patients[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msegpath \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas not processed correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/CISC471-882/patient_data.py:9\u001b[0m, in \u001b[0;36mPatient.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mct \u001b[38;5;241m=\u001b[39m ct_data\u001b[38;5;241m.\u001b[39mCt_Data()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# automatically assume this group is non cancerous\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mct\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/CISC471-882/patient_data.py:28\u001b[0m, in \u001b[0;36mPatient.load_data\u001b[0;34m(self, folder_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# check if this is folder, if so go into the folder\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CISC471-882/patient_data.py:28\u001b[0m, in \u001b[0;36mPatient.load_data\u001b[0;34m(self, folder_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# check if this is folder, if so go into the folder\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CISC471-882/patient_data.py:22\u001b[0m, in \u001b[0;36mPatient.load_data\u001b[0;34m(self, folder_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# check if this is a ct file (folder will have multiple files and this file will be a dcm)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m folder_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dcm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_dicoms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# check if this is an annotation file (folder should only have 1 file which will be a dcm)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m folder_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dcm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/CISC471-882/ct_data.py:13\u001b[0m, in \u001b[0;36mCt_Data.import_dicoms\u001b[0;34m(self, new_item)\u001b[0m\n\u001b[1;32m     11\u001b[0m item \u001b[38;5;241m=\u001b[39m pydicom\u001b[38;5;241m.\u001b[39mdcmread(new_item)\n\u001b[1;32m     12\u001b[0m uid \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mSOPInstanceUID\n\u001b[0;32m---> 13\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[uid] \u001b[38;5;241m=\u001b[39m img\n",
      "File \u001b[0;32m~/CISC471-882/ct_data.py:17\u001b[0m, in \u001b[0;36mCt_Data.load_images\u001b[0;34m(self, dicom)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, dicom):\n\u001b[0;32m---> 17\u001b[0m     pix_array \u001b[38;5;241m=\u001b[39m \u001b[43mdicom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_array\u001b[49m\n\u001b[1;32m     18\u001b[0m     img_normalized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(pix_array, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX)\n\u001b[1;32m     19\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(img_normalized)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/dataset.py:2193\u001b[0m, in \u001b[0;36mDataset.pixel_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpixel_array\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the pixel data as a :class:`numpy.ndarray`.\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \n\u001b[1;32m   2137\u001b[0m \u001b[38;5;124;03m    .. warning::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;124;03m        that iterates through the image frames.\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_pixel_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pixel_array)\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/dataset.py:1726\u001b[0m, in \u001b[0;36mDataset.convert_pixel_data\u001b[0;34m(self, handler_name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pdh\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;66;03m# Use 'pydicom.pixels' backend\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m     opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoding_plugin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m-> 1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pixel_array \u001b[38;5;241m=\u001b[39m \u001b[43mpixel_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pixel_id \u001b[38;5;241m=\u001b[39m get_image_pixel_ids(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;66;03m# Use 'pydicom.pixel_data_handlers' backend\u001b[39;00m\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/pixels/utils.py:1415\u001b[0m, in \u001b[0;36mpixel_array\u001b[0;34m(src, ds_out, specific_tags, index, raw, decoding_plugin, **kwargs)\u001b[0m\n\u001b[1;32m   1413\u001b[0m ds: Dataset \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m   1414\u001b[0m file_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_meta\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m-> 1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tsyntax \u001b[38;5;241m:=\u001b[39m \u001b[43mfile_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransferSyntaxUID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m):\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to decode the pixel data as the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_meta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no (0002,0010) \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer Syntax UID\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m element\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m     )\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/dataset.py:839\u001b[0m, in \u001b[0;36mDataset.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/dataset.py:909\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    907\u001b[0m tag \u001b[38;5;241m=\u001b[39m tag_for_keyword(name)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# None means `name` isn't a DICOM element keyword\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43mTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict:\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[tag]\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/conda-envs/venv2/lib/python3.12/site-packages/pydicom/tag.py:138\u001b[0m, in \u001b[0;36mTag\u001b[0;34m(arg, arg2)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m long_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create an element tag from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlong_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: tags must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe positive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using the patient_data data structure, load in all the patient data and save it in a dictionary with the folder name as the key\n",
    "def load_all_patients(path, add_label = False):\n",
    "    patients = np.array([])\n",
    "    folder = os.listdir(path)\n",
    "    for name in folder:\n",
    "        patients= np.append(patients, patient_data.Patient(os.path.join(path, name)))\n",
    "        if patients[-1].segpath == None:\n",
    "            print(name, \"was not processed correctly\")\n",
    "            patients = patients[:-1]\n",
    "        elif add_label:\n",
    "            patients[-1].label_imgs()\n",
    "    return patients\n",
    "\n",
    "patients = load_all_patients(cancerous_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nc = 0\n",
    "num_c = 0\n",
    "for i in patients:\n",
    "    num_nc += sum(1 for j in i.labels if j ==0)   \n",
    "    num_c += sum(1 for j in i.labels if j ==1)  \n",
    "\n",
    "print(\"number of patients loaded:\", len(patients))\n",
    "print(\"number of non-cancerous images in this dataset:\", num_nc)\n",
    "print(\"number of cancerous images in this dataset:\", num_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix up the data\n",
    "patients = shuffle(patients)\n",
    "\n",
    "# Train-test split should be 80-20. \n",
    "# Since the data has been shuffled, we can just grab the 1st 80% of the list and make it the train set and the remainder is the test set\n",
    "train_patients = patients[:math.floor(len(patients) * 0.8)]\n",
    "test_patients = patients[math.floor(len(patients) * 0.8):]\n",
    "\n",
    "# split the test patients up into images and labels\n",
    "x_test = []\n",
    "y_test = np.array([])\n",
    "for p in test_patients:\n",
    "    x_test.extend(y for y in p.ct.data.values())\n",
    "    y_test = np.append(y_test, p.labels)\n",
    "x_test = np.asarray(x_test)\n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "\n",
    "# split the train patients up into images and labels\n",
    "x_train, y_train = [], np.array([])\n",
    "for patient in train_patients:\n",
    "    x_train.extend(image for image in patient.ct.data.values())\n",
    "    y_train = np.append(y_train, patient.labels)\n",
    "x_train, y_train = shuffle(np.asarray(x_train), y_train)\n",
    "\n",
    "del train_patients\n",
    "del test_patients\n",
    "del patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom callback to clear any memory that is no longer being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(x_train, y_train)):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Load the ResNet50 model pre-trained on ImageNet\n",
    "    model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(512, 512, 3)))\n",
    "    \n",
    "    # Custom layers\n",
    "    flattened = tf.keras.layers.Flatten()(model.output)\n",
    "    l2 = tf.keras.layers.Dense(128, activation='relu')(flattened)\n",
    "    l3 = tf.keras.layers.Dense(1, activation='sigmoid')(l2)\n",
    "    \n",
    "    # Define the full model\n",
    "    model = tf.keras.models.Model(inputs=model.input, outputs=l3)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.00001),  # Use a smaller learning rate for end-to-end training\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Early stopping and learning rate scheduler\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "    \n",
    "    # Train the model directly using the training and validation data\n",
    "    history = model.fit(\n",
    "        x=x_train[train_idx],\n",
    "        y=y_train[train_idx],\n",
    "        validation_data=(x_train[val_idx], y_train[val_idx]),\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# x_data = np.array(x_c)/255  # Normalize the images\n",
    "# y_data = np.array(y_c)\n",
    "# Model training and evaluation loop\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_patients, [0]*len(train_patients))):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    p_train, p_val = train_patients[train_idx], train_patients[val_idx]\n",
    "    print('point a passed')\n",
    "\n",
    "#     # possible model to test\n",
    "#     model = Sequential([\n",
    "#         Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=64, kernel_size=(1, 1), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=1),\n",
    "#         Flatten(),\n",
    "#         Dense(units=1, activation='sigmoid')\n",
    "#     ])\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Flatten(),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print('point b passed')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = np.array([])\n",
    "    for p in p_train:\n",
    "        x_train.extend(y for y in p.ct.data.values())\n",
    "        y_train = np.append(y_train, p.labels)\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_train = np.asarray(x_train)\n",
    "\n",
    "    x_val = []\n",
    "    y_val = np.array([])\n",
    "    for p in p_val:\n",
    "        x_val.extend(y for y in p.ct.data.values())\n",
    "        y_val = np.append(y_val, p.labels)\n",
    "    x_val, y_val = shuffle(x_val, y_val)\n",
    "    x_val = np.asarray(x_val)\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = datagen.flow(x_train, y_train, batch_size=16)\n",
    "    val_generator = datagen.flow(x_val, y_val, batch_size=16)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        callbacks=[MyCustomCallback()],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model architecture built\")\n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))\n",
    "for i, j in enumerate(y_test):\n",
    "    k = predictions[i][0]\n",
    "    if j != k:\n",
    "        print(j, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_tests = 1\n",
    "# cnns = []\n",
    "# for i in range(num_tests):\n",
    "# cnns.append(cnn.CNN(x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cnns[0].test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
