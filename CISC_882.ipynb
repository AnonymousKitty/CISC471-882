{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 22:39:29.827539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 22:39:29.844897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733092769.864225 1224268 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733092769.870029 1224268 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 22:39:29.891171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "import patient_data\n",
    "import cnn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the folder path for the cancer and the non-cancer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = json.loads(open(\"./paths.json\").read())\n",
    "\n",
    "personal_path = all_paths['personal_path']\n",
    "non_cancerous_path = personal_path + all_paths['non_cancerous_path']\n",
    "cancerous_path = personal_path + all_paths['cancerous_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the DICOM files and preprocess/label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the patient_data data structure, load in all the patient data and save it in a dictionary with the folder name as the key\n",
    "def load_all_patients(path, add_label = False):\n",
    "    patients = {}\n",
    "    folder = os.listdir(path)\n",
    "    for name in folder:\n",
    "        patients[name] = patient_data.Patient(os.path.join(path, name))\n",
    "        if add_label:\n",
    "            if patients[name].segpath == None:\n",
    "                print(name, \"was not processed correctly\")\n",
    "                patients.pop(name)\n",
    "            else:\n",
    "                patients[name].label_imgs()\n",
    "    return patients\n",
    "\n",
    "# nc_patients = load_all_patients(non_cancerous_path)\n",
    "c_patients = load_all_patients(cancerous_path, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not sure if we need this\n",
    "# # create a list for the merged data\n",
    "# x = []\n",
    "# y = []\n",
    "\n",
    "# create a list for only the cancerous dataset data\n",
    "x_c = []\n",
    "y_c = []\n",
    "# # create a list for only the non-cancerous dataset data\n",
    "# x_nc = []\n",
    "# y_nc = []\n",
    "\n",
    "for patient in c_patients.values():\n",
    "    for i, img in enumerate(patient.ct.data.values()):\n",
    "        x_c.append(img)\n",
    "        y_c.append(patient.labels[i])\n",
    "        # # not sure if we need this\n",
    "        # x.append(img)\n",
    "        # y.append(patient.labels[i])\n",
    "\n",
    "x_c, y_c = shuffle(x_c, y_c)\n",
    "c_patients = None\n",
    "\n",
    "# for patient in nc_patients.values():\n",
    "#     for i, img in enumerate(patient.ct.images):\n",
    "#         x_nc.append(img)\n",
    "#         y_nc.append(patient.labels[i])\n",
    "#         # # not sure if we need this\n",
    "#         # x.append(img)\n",
    "#         # y.append(patient.labels[i])\n",
    "\n",
    "# # not sure if we need this\n",
    "# # Shuffle the merged data\n",
    "# combined = list(zip(x, y))\n",
    "# np.random.shuffle(combined)\n",
    "# x2, y2 = zip(*combined)\n",
    "\n",
    "# def generate_train_test():\n",
    "#     # to ensure equal distribution of non-cancer to cancer data, split the data before merging it\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x_c, y_c, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # x_train_add, x_test_add, y_train_add, y_test_add = train_test_split(x_nc, y_nc, test_size=0.2, random_state=42)\n",
    "#     # x_train.extend(x_train_add) \n",
    "#     # x_test.extend(x_test_add) \n",
    "#     # y_train.extend(y_train_add) \n",
    "#     # y_test.extend(y_test_add) \n",
    "\n",
    "#     # Convert lists to arrays\n",
    "#     x_train = np.array(x_train)/255\n",
    "#     x_test = np.array(x_test)/255\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_test = np.array(y_test)\n",
    "\n",
    "#     return x_train, x_test, y_train, y_test, x_train[0]\n",
    "\n",
    "# x_train, x_test, y_train, y_test, test = generate_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Num GPUs Available: ', len(physical_devices))\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model template as a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this function once ever, unless you chose to change the model\n",
    "if not os.path.isfile(\"models/model.json\"):\n",
    "    model = Sequential([\n",
    "            Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "            MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "            Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "            MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "            Flatten(),\n",
    "            Dense(units=1, activation='sigmoid')\n",
    "        ])\n",
    "    model = model.to_json()\n",
    "    with open(\"models/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1/5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# in case the user is importing the model from a previously saved JSON\n",
    "json_file = open('models/model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "x_data = np.array(x_c)/255  # Normalize the images\n",
    "y_data = np.array(y_c)\n",
    "# Model training and evaluation loop\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(x_c, y_c)):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    x_train, x_val = x_data[train_idx], x_data[val_idx]\n",
    "    y_train, y_val = y_data[train_idx], y_data[val_idx]\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=10,\n",
    "        epochs=20,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_val) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_val, predictions, output_dict=True)\n",
    "    print(classification_report(y_val, predictions))\n",
    "    # Save fold results\n",
    "    model.save(f\"models/kfold_{i}_model.keras\", fold)\n",
    "    model = None\n",
    "    fold_results.append(report)\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Grad-CAM function to compute heatmap\n",
    "def get_gradcam_heatmap(model, img_array, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Function to overlay Grad-CAM heatmap on original image\n",
    "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
    "    img = image.load_img(img_path)\n",
    "    img = image.img_to_array(img)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    jet = plt.cm.get_cmap(\"jet\")\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    jet_heatmap = image.array_to_img(jet_heatmap).resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = image.img_to_array(jet_heatmap)\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    return image.array_to_img(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency map computation function\n",
    "def compute_saliency_map(model, img_array, pred_index=None):\n",
    "    img_array = tf.convert_to_tensor(img_array)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img_array)\n",
    "        predictions = model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, pred_index]\n",
    "    grads = tape.gradient(loss, img_array)\n",
    "    saliency = tf.reduce_max(tf.abs(grads), axis=-1).numpy()\n",
    "    return saliency[0]\n",
    "\n",
    "# Function to display saliency map\n",
    "def display_saliency_map(img_array, saliency_map):\n",
    "    plt.imshow(saliency_map, cmap=\"viridis\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=10,\n",
    "        epochs=20,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_val) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_val, predictions, output_dict=True)\n",
    "    print(classification_report(y_val, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(512,512,1)),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=10,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/models/shri_model_1.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))\n",
    "for i, j in enumerate(y_test):\n",
    "    k = predictions[i][0]\n",
    "    if j != k:\n",
    "        print(j, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_tests = 1\n",
    "# cnns = []\n",
    "# for i in range(num_tests):\n",
    "# cnns.append(cnn.CNN(x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cnns[0].test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
