{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import patient_data\n",
    "import pydicom\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the folder path for the cancer and the non-cancer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = json.loads(open(\"./paths.json\").read())\n",
    "\n",
    "personal_path = all_paths['personal_path']\n",
    "cancerous_path = personal_path + all_paths['cancerous_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure GPUs is applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Num GPUs Available: ', len(physical_devices))\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the DICOM files and preprocess/label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the patient_data data structure, load in all the patient data and save it in a dictionary with the folder name as the key\n",
    "def load_all_patients(path, add_label = False):\n",
    "    patients = np.array([])\n",
    "    folder = os.listdir(path)\n",
    "    # make shuffling and train/test split here\n",
    "    for name in folder:\n",
    "        patients= np.append(patients, patient_data.Patient(path, name, add_label))\n",
    "        if patients[-1].ct_paths == []:\n",
    "            print(name, \"was not processed correctly\")\n",
    "            patients = patients[:-1]\n",
    "    return patients\n",
    "\n",
    "patients = load_all_patients(cancerous_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients loaded: 3\n",
      "number of non-cancerous images in this dataset: 293\n",
      "number of cancerous images in this dataset: 53\n"
     ]
    }
   ],
   "source": [
    "num_nc = 0\n",
    "num_c = 0\n",
    "for i in patients:\n",
    "    num_nc += sum(1 for j in i.labels if j ==0)   \n",
    "    num_c += sum(1 for j in i.labels if j ==1)  \n",
    "\n",
    "print(\"number of patients loaded:\", len(patients))\n",
    "print(\"number of non-cancerous images in this dataset:\", num_nc)\n",
    "print(\"number of cancerous images in this dataset:\", num_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix up the data\n",
    "patients = shuffle(patients)\n",
    "\n",
    "# Train-test split should be 80-20. \n",
    "# Since the data has been shuffled, we can just grab the 1st 80% of the list and make it the train set and the remainder is the test set\n",
    "train_patients = patients[:math.floor(len(patients) * 0.8)]\n",
    "test_patients = patients[math.floor(len(patients) * 0.8):]\n",
    "\n",
    "def save_imgs(dest, list_paths, list_labels):\n",
    "    cancerous_img_folder = dest + \"/cancerous\"\n",
    "    non_cancerous_img_folder = dest + \"/non_cancerous\"\n",
    "    if not os.path.exists(cancerous_img_folder):\n",
    "        os.makedirs(cancerous_img_folder)\n",
    "    if not os.path.exists(non_cancerous_img_folder):\n",
    "        os.makedirs(non_cancerous_img_folder)\n",
    "    # for our purposes, we know we dont want these folders to have any previous content, so remove anything in here if there is something\n",
    "    for file in glob.glob(cancerous_img_folder + \"/*\"):\n",
    "        os.remove(file)\n",
    "    for file in glob.glob(non_cancerous_img_folder + \"/*\"):\n",
    "        os.remove(file)\n",
    "    # save images\n",
    "    for i, dicom_path in enumerate(list_paths):\n",
    "        if list_labels[i] == 0:\n",
    "            img_folder = non_cancerous_img_folder\n",
    "        else:\n",
    "            img_folder = cancerous_img_folder\n",
    "        dicom = pydicom.dcmread(dicom_path)\n",
    "        pix_array = dicom.pixel_array\n",
    "        img = cv2.normalize(pix_array, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        cv2.imwrite(f\"{img_folder}/image_{i}.jpg\", img)\n",
    "\n",
    "train_dest = personal_path + \"/train/train\"\n",
    "val_dest = personal_path + \"/train/validation\"\n",
    "test_dest = personal_path + \"/test\"\n",
    "\n",
    "\n",
    "# save_imgs(train_dest, train_patients)\n",
    "x_test, y_test = [], []\n",
    "for patient in test_patients:\n",
    "    x_test.extend(patient.ct_paths)\n",
    "    y_test.extend(patient.labels)\n",
    "save_imgs(test_dest, x_test, y_test)\n",
    "test_ds = ImageDataGenerator().flow_from_directory(\n",
    "        directory=test_dest, # the path to the test directory.\n",
    "        target_size=(512, 512),\n",
    "        batch_size=32,\n",
    "        class_mode=\"binary\",\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "# split the train patients up into images and labels\n",
    "x_train, y_train = [], []\n",
    "for patient in train_patients:\n",
    "    x_train.extend(patient.ct_paths)\n",
    "    y_train.extend(patient.labels)\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "del train_patients\n",
    "del test_patients\n",
    "del patients\n",
    "# del x_test\n",
    "# del y_test\n",
    "# del x_train\n",
    "# del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom callback to clear any memory that is no longer being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1/5\n",
      "Found 188 images belonging to 2 classes.\n",
      "Found 47 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_712']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(x_train, y_train)):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    x_train_list, y_train_list = x_train[train_idx], y_train[train_idx]\n",
    "    x_val_list, y_val_list = x_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    save_imgs(train_dest, x_train_list, y_train_list)\n",
    "    save_imgs(val_dest, x_val_list, y_val_list)   \n",
    "        \n",
    "    # Load the ResNet50 model pre-trained on ImageNet\n",
    "    model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(512, 512, 3)))\n",
    "    \n",
    "    # Custom layers\n",
    "    flattened = tf.keras.layers.Flatten()(model.output)\n",
    "    l2 = tf.keras.layers.Dense(128, activation='relu')(flattened)\n",
    "    l3 = tf.keras.layers.Dense(1, activation='sigmoid')(l2)\n",
    "    \n",
    "    # Define the full model\n",
    "    model = tf.keras.models.Model(inputs=model.input, outputs=l3)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.00001),  # Use a smaller learning rate for end-to-end training\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    train_ds = ImageDataGenerator().flow_from_directory(\n",
    "            directory=train_dest, # the path to the training directory.\n",
    "            target_size=(512, 512),\n",
    "            batch_size=32,\n",
    "            class_mode=\"binary\",\n",
    "        )\n",
    "    val_ds = ImageDataGenerator().flow_from_directory(\n",
    "            directory=val_dest, # the path to the validation directory.\n",
    "            target_size=(512, 512),\n",
    "            batch_size=32,\n",
    "            class_mode=\"binary\",\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Early stopping and learning rate scheduler\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "    \n",
    "    # Train the model directly using the training and validation data\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(test_ds) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(test_ds.classes, predictions, output_dict=True)\n",
    "    print(classification_report(test_ds.classes, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# x_data = np.array(x_c)/255  # Normalize the images\n",
    "# y_data = np.array(y_c)\n",
    "# Model training and evaluation loop\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_patients, [0]*len(train_patients))):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    p_train, p_val = train_patients[train_idx], train_patients[val_idx]\n",
    "    print('point a passed')\n",
    "\n",
    "#     # possible model to test\n",
    "#     model = Sequential([\n",
    "#         Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=64, kernel_size=(1, 1), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=1),\n",
    "#         Flatten(),\n",
    "#         Dense(units=1, activation='sigmoid')\n",
    "#     ])\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Flatten(),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print('point b passed')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = np.array([])\n",
    "    for p in p_train:\n",
    "        x_train.extend(y for y in p.ct.data.values())\n",
    "        y_train = np.append(y_train, p.labels)\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_train = np.asarray(x_train)\n",
    "\n",
    "    x_val = []\n",
    "    y_val = np.array([])\n",
    "    for p in p_val:\n",
    "        x_val.extend(y for y in p.ct.data.values())\n",
    "        y_val = np.append(y_val, p.labels)\n",
    "    x_val, y_val = shuffle(x_val, y_val)\n",
    "    x_val = np.asarray(x_val)\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = datagen.flow(x_train, y_train, batch_size=16)\n",
    "    val_generator = datagen.flow(x_val, y_val, batch_size=16)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        callbacks=[MyCustomCallback()],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model architecture built\")\n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))\n",
    "for i, j in enumerate(y_test):\n",
    "    k = predictions[i][0]\n",
    "    if j != k:\n",
    "        print(j, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_tests = 1\n",
    "# cnns = []\n",
    "# for i in range(num_tests):\n",
    "# cnns.append(cnn.CNN(x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cnns[0].test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
