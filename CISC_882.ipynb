{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 01:24:43.439219: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 01:24:43.456055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734225883.475046   34316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734225883.480736   34316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 01:24:43.500824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import patient_data\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the folder path for the cancer and the non-cancer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = json.loads(open(\"./paths.json\").read())\n",
    "\n",
    "personal_path = all_paths['personal_path']\n",
    "cancerous_path = personal_path + all_paths['cancerous_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure GPUs is applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Num GPUs Available: ', len(physical_devices))\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the DICOM files and preprocess/label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints was not processed correctly\n"
     ]
    }
   ],
   "source": [
    "# Using the patient_data data structure, load in all the patient data and save it in a dictionary with the folder name as the key\n",
    "def load_all_patients(path, add_label = False):\n",
    "    patients = np.array([])\n",
    "    folder = os.listdir(path)\n",
    "    for name in folder:\n",
    "        patients= np.append(patients, patient_data.Patient(os.path.join(path, name)))\n",
    "        if patients[-1].segpath == None:\n",
    "            print(name, \"was not processed correctly\")\n",
    "            patients = patients[:-1]\n",
    "        elif add_label:\n",
    "            patients[-1].label_imgs()\n",
    "#             print(name, \"has been processed\")\n",
    "    return patients\n",
    "\n",
    "patients = load_all_patients(cancerous_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dicom_image_generator(object):\n",
    "\n",
    "    def __init__(self, n, labels):\n",
    "        self.n = [i.decode(\"utf-8\") for i in n]\n",
    "        self.labels = labels.astype('int32')\n",
    "        self.num = 0\n",
    "        self.image = self.load_image()\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        if self.num < len(self.n):\n",
    "            img = self.image\n",
    "            l = self.labels[self.num]\n",
    "            self.num = self.num+1\n",
    "            return {\"x_train_data\" : img}, {\"x_train_label\" : l}\n",
    "        raise StopIteration()\n",
    "    \n",
    "    def __call__(self):\n",
    "        yield {\"x_train_data\" : self.image}, {\"x_train_label\" : self.labels[self.num]}\n",
    "\n",
    "    def load_image(self):\n",
    "        dicom = pydicom.dcmread(self.n[self.num])\n",
    "        pix_array = dicom.pixel_array\n",
    "        img_normalized = cv2.normalize(pix_array, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        img_array = np.asarray(img_normalized).astype('float32')/255\n",
    "        return img_normalized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patients loaded: 166\n",
      "number of non-cancerous images in this dataset: 17440\n",
      "number of cancerous images in this dataset: 3220\n"
     ]
    }
   ],
   "source": [
    "num_nc = 0\n",
    "num_c = 0\n",
    "for i in patients:\n",
    "    num_nc += sum(1 for j in i.labels if j ==0)   \n",
    "    num_c += sum(1 for j in i.labels if j ==1)  \n",
    "\n",
    "print(\"number of patients loaded:\", len(patients))\n",
    "print(\"number of non-cancerous images in this dataset:\", num_nc)\n",
    "print(\"number of cancerous images in this dataset:\", num_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix up the data\n",
    "patients = shuffle(patients)\n",
    "\n",
    "# Train-test split should be 80-20. \n",
    "# Since the data has been shuffled, we can just grab the 1st 80% of the list and make it the train set and the remainder is the test set\n",
    "train_patients = patients[:math.floor(len(patients) * 0.8)]\n",
    "test_patients = patients[math.floor(len(patients) * 0.8):]\n",
    "\n",
    "# split the test patients up into images and labels\n",
    "x_test = []\n",
    "y_test = np.array([])\n",
    "for p in test_patients:\n",
    "    x_test.extend(p.ctpaths)\n",
    "    y_test = np.append(y_test, p.labels)\n",
    "\n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "train_data = tf.data.Dataset.from_generator(dicom_image_generator,\n",
    "                                                args=[x_test, y_test],\n",
    "                                                output_signature=(\n",
    "                                                    {\"x_train_data\" : tf.TensorSpec(shape=(512, 512), dtype=tf.int16)},\n",
    "                                                    {\"x_train_label\" : tf.TensorSpec(shape=(), dtype=np.int32)}\n",
    "                                                )\n",
    "                                            )\n",
    "\n",
    "# split the train patients up into images and labels\n",
    "x_train, y_train = [], np.array([])\n",
    "for patient in train_patients:\n",
    "    x_train.extend(patient.ctpaths)\n",
    "    y_train = np.append(y_train, patient.labels)\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "test_data = tf.data.Dataset.from_generator(dicom_image_generator,\n",
    "                                                args=[x_test, y_test],\n",
    "                                                output_signature=(\n",
    "                                                    {\"x_train_data\" : tf.TensorSpec(shape=(512, 512), dtype=tf.int16)},\n",
    "                                                    {\"x_train_label\" : tf.TensorSpec(shape=(), dtype=np.int32)}\n",
    "                                                )\n",
    "                                            )\n",
    "\n",
    "# del train_patients\n",
    "# del test_patients\n",
    "# del patients\n",
    "# del x_test\n",
    "# del y_test\n",
    "# del x_train\n",
    "# del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fb3fc5d2680>\n",
      "dict_values([<tf.Tensor: shape=(512, 512), dtype=int16, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int16)>])\n",
      "<generator object <genexpr> at 0x7fb3fc5d25c0>\n",
      "dict_values([<tf.Tensor: shape=(512, 512), dtype=int16, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int16)>])\n",
      "<generator object <genexpr> at 0x7fb3fc5d13c0>\n",
      "dict_values([<tf.Tensor: shape=(512, 512), dtype=int16, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int16)>])\n"
     ]
    }
   ],
   "source": [
    "b = None\n",
    "for a in train_data.take(3):\n",
    "    if a[0].values() == b:\n",
    "        print(\"hmm\")\n",
    "    else:\n",
    "        print(x for x in a[0].values())\n",
    "        b = a[0].values()\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom callback to clear any memory that is no longer being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(x_train, y_train)):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Load the ResNet50 model pre-trained on ImageNet\n",
    "    model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(512, 512, 3)))\n",
    "    \n",
    "    # Custom layers\n",
    "    flattened = tf.keras.layers.Flatten()(model.output)\n",
    "    l2 = tf.keras.layers.Dense(128, activation='relu')(flattened)\n",
    "    l3 = tf.keras.layers.Dense(1, activation='sigmoid')(l2)\n",
    "    \n",
    "    # Define the full model\n",
    "    model = tf.keras.models.Model(inputs=model.input, outputs=l3)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.00001),  # Use a smaller learning rate for end-to-end training\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Early stopping and learning rate scheduler\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "    \n",
    "    # Train the model directly using the training and validation data\n",
    "    history = model.fit(\n",
    "        x=x_train[train_idx],\n",
    "        y=y_train[train_idx],\n",
    "        validation_data=(x_train[val_idx], y_train[val_idx]),\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# x_data = np.array(x_c)/255  # Normalize the images\n",
    "# y_data = np.array(y_c)\n",
    "# Model training and evaluation loop\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_patients, [0]*len(train_patients))):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    p_train, p_val = train_patients[train_idx], train_patients[val_idx]\n",
    "    print('point a passed')\n",
    "\n",
    "#     # possible model to test\n",
    "#     model = Sequential([\n",
    "#         Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "#         Conv2D(filters=64, kernel_size=(1, 1), activation='relu', padding='same'),\n",
    "#         MaxPool2D(pool_size=(2, 2), strides=1),\n",
    "#         Flatten(),\n",
    "#         Dense(units=1, activation='sigmoid')\n",
    "#     ])\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "        Flatten(),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print('point b passed')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = np.array([])\n",
    "    for p in p_train:\n",
    "        x_train.extend(y for y in p.ct.data.values())\n",
    "        y_train = np.append(y_train, p.labels)\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_train = np.asarray(x_train)\n",
    "\n",
    "    x_val = []\n",
    "    y_val = np.array([])\n",
    "    for p in p_val:\n",
    "        x_val.extend(y for y in p.ct.data.values())\n",
    "        y_val = np.append(y_val, p.labels)\n",
    "    x_val, y_val = shuffle(x_val, y_val)\n",
    "    x_val = np.asarray(x_val)\n",
    "    \n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    train_generator = datagen.flow(x_train, y_train, batch_size=16)\n",
    "    val_generator = datagen.flow(x_val, y_val, batch_size=16)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        callbacks=[MyCustomCallback()],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model architecture built\")\n",
    "    # Evaluate the model\n",
    "    predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_results.append(report)\n",
    "    K.clear_session()\n",
    "\n",
    "# Aggregate results\n",
    "avg_accuracy = np.mean([fold['accuracy'] for fold in fold_results])\n",
    "print(f\"\\nAverage Accuracy Across {n_splits} Folds: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))\n",
    "for i, j in enumerate(y_test):\n",
    "    k = predictions[i][0]\n",
    "    if j != k:\n",
    "        print(j, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_tests = 1\n",
    "# cnns = []\n",
    "# for i in range(num_tests):\n",
    "# cnns.append(cnn.CNN(x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cnns[0].test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
